Please help with your inputs on Exam test questions

-------------------------------------------------------------------------------------

1) Find the pod that consumes the most CPU in all namespace(including kube-system) in all cluster(currently we have single cluster). Then, store the result in the file high_cpu_pod.txt with the following format: pod_name,namespace .

Run top command for pod (for all pods under all namespaces)

kubectl top pods --all-namespaces 

If asked for sort use below command

kubectl top pods --sort-by=cpu or mem --all-namespaces

store the output in file- high_cpu_pod.txt with the following format: pod_name,namespace

How to format store?
>>controlplane $ kubectl get pods -A -o custom-columns=POD-NAME:.metadata.name,NAMESPACE:.metadata.namespace > pod.txt
controlplane $ cat pod.txt
POD-NAME                                   NAMESPACE
nginx-bf5d5cf98-f8hp8                      default
nginx-bf5d5cf98-kgrls                      default
calico-kube-controllers-75bdb5b75d-2b6mr   kube-system
canal-q652m                                kube-system
canal-wzjz6                                kube-system
coredns-5c69dbb7bd-6xvhl                   kube-system

** SEE if this is how you want...
-------------------------------------------------------------------------------------

2) Create a Kubernetes Pod configuration to facilitate real-time monitoring of a log file. Specifically, you need to set up a Pod named alpine-pod-pod that runs an Alpine Linux container.

Requirements:

Name the Pod alpine-pod-pod
Use alpine:latest image
Container name alpine-container
Configure the container to execute the tail -f /config/log.txt command(using args ) with /bin/sh (using command ) to continuously monitor and display the contents of a log file.

Created alpine pod is going in "Completed" status or ImagePullBackoff.
** TRY THIS

controlplane $ cat alpine.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: alpine-pod-pod
spec:
  containers:
  - name: alpine-container
    image: alpine:latest
    command: ["/bin/sh"] ## command to run shell inside container
    args:   [ "-c", "while true; do tail -f /config/log.txt; sleep 10;done" ]
           # args: ["-c" >> command to run in shell
           # tail will work only single time and conatiner would stop so need to loop it so while loop 

controlplane $ kubectl get pods | grep alpine
alpine-pod-pod          1/1     Running   0          3m35s


-------------------------------------------------------------------------------------------------------------------------

Question No- 3 (Killercoda Link- https://killercoda.com/sachin/course/CKA/deployment-history)

The deployment named video-app has experienced multiple rolling updates and rollbacks. Your task is to total revision of this deployment and record the image name used in 3rd revision to file app-file.txt in this format REVISION_TOTAL_COUNT,IMAGE_NAME .

Deployment yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "3"
  creationTimestamp: "2024-08-12T16:25:12Z"
  generation: 3
  labels:
    app: video-app
  name: video-app
  namespace: default
  resourceVersion: "2490"
  uid: 795fc837-1a7b-4b78-b066-c1e70806d2b9
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: video-app
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
	   app: video-app
    spec:
      containers:
      - image: redis:7.0.13
        imagePullPolicy: IfNotPresent
        name: redis
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2024-08-12T16:25:27Z"
    lastUpdateTime: "2024-08-12T16:25:27Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2024-08-12T16:25:12Z"
    lastUpdateTime: "2024-08-12T16:25:42Z"
    message: ReplicaSet "video-app-7f5bc58cd6" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
	 observedGeneration: 3
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1

- Command to collect the output in file using REVISION_TOTAL_COUNT:.metadata.generation
kubectl get deployment video-app -o custom-columns=REVISION_TOTAL_COUNT:.metadata.generation,IMAGE_NAME:.'.spec.containers[0].image' > app-file.txt
 
cat app-file.txt
 
REVISION_TOTAL_COUNT   IMAGE_NAME
3                      redis:7.0.13

- Command to collect the output in file using REVISION_TOTAL_COUNT:.status.observedGeneration
kubectl get deployment video-app -o custom-columns=REVISION_TOTAL_COUNT:.status.observedGeneration,IMAGE_NAME:.'.spec
.containers[0].image' > app-file.txt

cat app-file.txt 
REVISION_TOTAL_COUNT   IMAGE_NAME
3                      redis:7.0.13

ISSUE- Using both metadata.generation and status.observedGeneration still validation fails of this question on the KillrKoda. Am I missing something in collecting this output?

TRAINER INPUTS:

??
******************************************************************************************************************
controlplane $ kubectl config use-context kubernetes-admin@kubernetes
Switched to context "kubernetes-admin@kubernetes".
controlplane $ kubectl rollout history deployment video-app
deployment.apps/video-app 
REVISION  CHANGE-CAUSE
1         <none>
2         <none>
3         <none>

controlplane $ kubectl rollout history deployment video-app --revision=3
controlplane $ echo "3,redis:7.0.13" > app-file.txt
controlplane $ cat app-file.txt 
3,redis:7.0.13

>> This way its validating
*******************************************************************************************************************


 
------------------------------------------------------------------------------------------------------------------------

Question No- 4 (Killercoda Link- https://killercoda.com/sachin/course/CKA/rollback)

Due to a missing feature in the current version. To resolve this issue, perform a rollback of the deployment redis-deployment to the previous version. After rolling back the deployment, save the image currently in use to the rolling-back-image.txt file, and finally increase the replica count to 3 

kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
redis-deployment   1/1     1            1           24s

kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
redis-deployment-66c8689579   1         1         1       26s
redis-deployment-66f7989d     0         0         0       29s

kubectl describe deployment redis-deployment
Name:                   redis-deployment
Namespace:              default
CreationTimestamp:      Wed, 14 Aug 2024 14:34:25 +0000
Labels:                 app=redis-deployment
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=redis-deployment
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=redis-deployment
  Containers:
   redis:
    Image:         redis:7.2.1
    Port:          <none>
    Host Port:     <none>
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  redis-deployment-66f7989d (0/0 replicas created)
NewReplicaSet:   redis-deployment-66c8689579 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42s   deployment-controller  Scaled up replica set redis-deployment-66f7989d to 1
  Normal  ScalingReplicaSet  39s   deployment-controller  Scaled up replica set redis-deployment-66c8689579 to 1
  Normal  ScalingReplicaSet  27s   deployment-controller  Scaled down replica set redis-deployment-66f7989d to 0 from 1

kubectl rollout undo deployment redis-deployment
deployment.apps/redis-deployment rolled back

kubectl rollout status deployment redis-deployment
deployment "redis-deployment" successfully rolled out

kubectl describe redis-deployment
error: the server doesn't have a resource type "redis-deployment"
controlplane $ kubectl describe deployment redis-deployment 
Name:                   redis-deployment
Namespace:              default
CreationTimestamp:      Wed, 14 Aug 2024 14:34:25 +0000
Labels:                 app=redis-deployment
Annotations:            deployment.kubernetes.io/revision: 3
Selector:               app=redis-deployment
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=redis-deployment
  Containers:
   redis:
    Image:         redis:7.0.13
    Port:          <none>
    Host Port:     <none>
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  redis-deployment-66c8689579 (0/0 replicas created)
NewReplicaSet:   redis-deployment-66f7989d (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  107s  deployment-controller  Scaled up replica set redis-deployment-66f7989d to 1
  Normal  ScalingReplicaSet  104s  deployment-controller  Scaled up replica set redis-deployment-66c8689579 to 1
  Normal  ScalingReplicaSet  92s   deployment-controller  Scaled down replica set redis-deployment-66f7989d to 0 from 1
  Normal  ScalingReplicaSet  39s   deployment-controller  Scaled up replica set redis-deployment-66f7989d to 1 from 0
  Normal  ScalingReplicaSet  36s   deployment-controller  Scaled down replica set redis-deployment-66c8689579 to 0 from 1

kubectl get deployment redis-deployment -o custom-columns=IMAGE_NAME:.'.spec.containers[0].image' > rolling-back-image.txt

cat rolling-back-image.txt 

Output

IMAGE_NAME
redis:7.0.13

kubectl scale deployment redis-deployment --replicas=3
deployment.apps/redis-deployment scaled

kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
redis-deployment   3/3     3            3           2m53s

kubectl describe deployment redis-deployment
Name:                   redis-deployment
Namespace:              default
CreationTimestamp:      Wed, 14 Aug 2024 14:34:25 +0000
Labels:                 app=redis-deployment
Annotations:            deployment.kubernetes.io/revision: 3
Selector:               app=redis-deployment
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=redis-deployment
  Containers:
   redis:
    Image:         redis:7.0.13
    Port:          <none>
    Host Port:     <none>
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  redis-deployment-66c8689579 (0/0 replicas created)
NewReplicaSet:   redis-deployment-66f7989d (3/3 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  5m22s  deployment-controller  Scaled up replica set redis-deployment-66f7989d to 1
  Normal  ScalingReplicaSet  5m19s  deployment-controller  Scaled up replica set redis-deployment-66c8689579 to 1
  Normal  ScalingReplicaSet  5m7s   deployment-controller  Scaled down replica set redis-deployment-66f7989d to 0 from 1
  Normal  ScalingReplicaSet  4m14s  deployment-controller  Scaled up replica set redis-deployment-66f7989d to 1 from 0
  Normal  ScalingReplicaSet  4m11s  deployment-controller  Scaled down replica set redis-deployment-66c8689579 to 0 from 1
  Normal  ScalingReplicaSet  2m37s  deployment-controller  Scaled up replica set redis-deployment-66f7989d to 3 from 1

ISSUE- Validation for this question still failed in KillrCoda. Am I missing some step or incorrect steps followed in answering this question?

TRAINER INPUTS:

??

------------------------------------------------------------------------------------------------------------------------

Question No- 5 (Killercoda Link- https://killercoda.com/sachin/course/CKA/deployment-rollout)

Question-
Create a new deployment named cache-deployment in the default namespace using a custom image redis:7.0.13 . Ensure that the deployment has the following specifications:

Set the replica count to 2 .
Set the strategy type RollingUpdate
Configure the MaxUnavailable field to 30% and the MaxSurge field to 45% .
Deploy the cache-deployment deployment and ensure that all pods are in a ready state.
Now, Perform an image upgrade to redis:7.2.1 .
Examine the rolling history of the deployment, and save the Total revision count to the total-revision.txt .

- Check the exisitng deployment
kubectl get deployments
Output

No resources found in default namespace.

- Create the deployment first with with running pods
kubectl create deployment cache-deployment --image=redis:7.0.13 --replicas=2

Output
deployment.apps/cache-deployment created

-Check the created deployment
kubectl get deployments

Output
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
cache-deployment   2/2     2            2           13s

- Create the yaml from the above created deployment
kubectl get deployments cache-deployment -o yaml > dp.yaml

- Now delete the deployment created
kubectl delete deployment cache-deployment

Output
deployment.apps "cache-deployment" deleted

- Edit the created dp.yaml to update values of the MaxUnavailable field to 30% and the MaxSurge field to 45% 
vi dp.yaml 

-Now apply the updated dp.yaml file to create deployment
kubectl apply -f dp.yaml 

Output
deployment.apps/cache-deployment created

- Check the created deployment
kubectl get deployments

Output
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
cache-deployment   2/2     2            2           5s

- Describe the created deployemnt to check the updated MaxUnavailable field to 30% and the MaxSurge field to 45% values

kubectl describe deployment cache-deployment

Output
Name:                   cache-deployment
Namespace:              default
CreationTimestamp:      Wed, 14 Aug 2024 16:39:25 +0000
Labels:                 app=cache-deployment
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=cache-deployment
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  30% max unavailable, 45% max surge
Pod Template:
  Labels:  app=cache-deployment
  Containers:
   redis:
    Image:         redis:7.0.13
    Port:          <none>
    Host Port:     <none>
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   cache-deployment-677589fbdc (2/2 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  18s   deployment-controller  Scaled up replica set cache-deployment-677589fbdc to 2

- Update the image of deployment as mention in instructions
kubectl set image deployment cache-deployment redis=redis:7.2.1

Output
deployment.apps/cache-deployment image updated

- Check the rollout status of the deployment
kubectl rollout status deployment cache-deployment

Output
deployment "cache-deployment" successfully rolled out

- Check the pods running
kubectl get pods

Output
NAME                                READY   STATUS    RESTARTS   AGE
cache-deployment-857c5c455b-c7nwp   1/1     Running   0          22s
cache-deployment-857c5c455b-jdq2j   1/1     Running   0          29s

- Describe both the pods to make sure images is updated

kubectl describe pod cache-deployment-857c5c455b-c7nwp

Output
Name:             cache-deployment-857c5c455b-c7nwp
Namespace:        default
Priority:         0
Service Account:  default
Node:             node01/172.30.2.2
Start Time:       Wed, 14 Aug 2024 16:40:31 +0000
Labels:           app=cache-deployment
                  pod-template-hash=857c5c455b
Annotations:      cni.projectcalico.org/containerID: 01a1855a6530481eb14838eaf891fc235bad1b158707631f7956a80614213dab
                  cni.projectcalico.org/podIP: 192.168.1.9/32
                  cni.projectcalico.org/podIPs: 192.168.1.9/32
Status:           Running
IP:               192.168.1.9
IPs:
  IP:           192.168.1.9
Controlled By:  ReplicaSet/cache-deployment-857c5c455b
Containers:
  redis:
    Container ID:   containerd://c5bab607b49983545fc1380ff11a0662adf6c5a97a8c7b2e01736fd4f2ae3aa8
    Image:          redis:7.2.1
    Image ID:       docker.io/library/redis@sha256:4ca2a277f1dc3ddd0da33a258096de9a1cf5b9d9bd96b27ee78763ee2248c28c
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Wed, 14 Aug 2024 16:40:32 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pdnqk (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-pdnqk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  18m   default-scheduler  Successfully assigned default/cache-deployment-857c5c455b-c7nwp to node01
  Normal  Pulled     18m   kubelet            Container image "redis:7.2.1" already present on machine
  Normal  Created    18m   kubelet            Created container redis
  Normal  Started    18m   kubelet            Started container redis

kubectl describe pod cache-deployment-857c5c455b-jdq2j

Output
Name:             cache-deployment-857c5c455b-jdq2j
Namespace:        default
Priority:         0
Service Account:  default
Node:             node01/172.30.2.2
Start Time:       Wed, 14 Aug 2024 16:40:23 +0000
Labels:           app=cache-deployment
                  pod-template-hash=857c5c455b
Annotations:      cni.projectcalico.org/containerID: f90013a4c54722583b44c4a6d0ceff2ebdf8959472023720bfcf82b34a36421f
                  cni.projectcalico.org/podIP: 192.168.1.8/32
                  cni.projectcalico.org/podIPs: 192.168.1.8/32
Status:           Running
IP:               192.168.1.8
IPs:
  IP:           192.168.1.8
Controlled By:  ReplicaSet/cache-deployment-857c5c455b
Containers:
  redis:
    Container ID:   containerd://948818fd39825e484275f7e2f60fb77807c61e9f02263c9c921f44859120b208
    Image:          redis:7.2.1
    Image ID:       docker.io/library/redis@sha256:4ca2a277f1dc3ddd0da33a258096de9a1cf5b9d9bd96b27ee78763ee2248c28c
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Wed, 14 Aug 2024 16:40:29 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dbmzt (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-dbmzt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  19m   default-scheduler  Successfully assigned default/cache-deployment-857c5c455b-jdq2j to node01
  Normal  Pulling    19m   kubelet            Pulling image "redis:7.2.1"
  Normal  Pulled     19m   kubelet            Successfully pulled image "redis:7.2.1" in 4.933s (4.933s including waiting). Image size: 51443557 bytes.
  Normal  Created    19m   kubelet            Created container redis
  Normal  Started    19m   kubelet            Started container redis

- Check the rollout history of deployment
kubectl rollout history deployment cache-deployment

Output
REVISION  CHANGE-CAUSE
1         <none>
2         <none>

- Check the REVISION detail of deployment
kubectl rollout history deployment cache-deployment --revision=1 

Output
Pod Template:
  Labels:       app=cache-deployment
        pod-template-hash=677589fbdc
  Containers:
   redis:
    Image:      redis:7.0.13
    Port:       <none>
    Host Port:  <none>
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>
  Node-Selectors:       <none>
  Tolerations:  <none>

kubectl rollout history deployment cache-deployment --revision=2

Output
Pod Template:
  Labels:       app=cache-deployment
        pod-template-hash=857c5c455b
  Containers:
   redis:
    Image:      redis:7.2.1
    Port:       <none>
    Host Port:  <none>
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>
  Node-Selectors:       <none>
  Tolerations:  <none>

- Now save total revision output to mention file-  total-revision.txt
kubectl get deployment cache-deployment -o custom-columns=REVISION:.status.observedGeneration > total-revision.txt

- Check the created total-revision.txt
cat total-revision.txt

Output
REVISION
2

ISSUE- Is the command use for the REVISION is correct or this output should show the revision like below
       
       Output
       REVISION
       1         
       2
       Can you please help to get command right if above output is expected.

TRAINER INPUTS:

??

--------------------------------------------------------------------------------------------------------------------

Question No- 6 (Killercoda Link- https://killercoda.com/sachin/course/CKA/etcd-backup-issue)

Question-
something is not working at the moment on controlplane node(Cause NotReady state), check that and etcd-controlplane pod is running in kube-system environment, take backup and store it in /opt/cluster_backup.db file, and also store backup console output store it in backup.txt

ssh controlplane

- Check the nodes

Kubectl get nodes

Output
NAME           STATUS     ROLES           AGE   VERSION
controlplane   NotReady   control-plane   12d   v1.30.0
node01         Ready      <none>          12d   v1.30.0

- Check describe node
kubectl describe node controlplane

output
Name:               controlplane
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=controlplane
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"c2:73:4f:c6:c9:07"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 172.30.1.2
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 172.30.1.2/24
                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.0.1
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 02 Aug 2024 07:24:51 +0000
Taints:             node.kubernetes.io/unreachable:NoExecute
                    node-role.kubernetes.io/control-plane:NoSchedule
                    node.kubernetes.io/unreachable:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  controlplane
  AcquireTime:     <unset>
  RenewTime:       Thu, 15 Aug 2024 06:18:59 +0000
Conditions:
  Type                 Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message
  ----                 ------    -----------------                 ------------------                ------              -------
  NetworkUnavailable   False     Thu, 15 Aug 2024 06:16:17 +0000   Thu, 15 Aug 2024 06:16:17 +0000   FlannelIsUp         Flannel is running on this node
  MemoryPressure       Unknown   Thu, 15 Aug 2024 06:16:05 +0000   Thu, 15 Aug 2024 06:19:40 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  DiskPressure         Unknown   Thu, 15 Aug 2024 06:16:05 +0000   Thu, 15 Aug 2024 06:19:40 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  PIDPressure          Unknown   Thu, 15 Aug 2024 06:16:05 +0000   Thu, 15 Aug 2024 06:19:40 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  Ready                Unknown   Thu, 15 Aug 2024 06:16:05 +0000   Thu, 15 Aug 2024 06:19:40 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
Addresses:
  InternalIP:  172.30.1.2
  Hostname:    controlplane
Capacity:
  cpu:                1
  ephemeral-storage:  20134592Ki
  hugepages-2Mi:      0
  memory:             2030940Ki
  pods:               110
Allocatable:
  cpu:                1
  ephemeral-storage:  19586931083
  hugepages-2Mi:      0
  memory:             1928540Ki
  pods:               110
System Info:
  Machine ID:                 388a2d0f867a4404bc12a0093bd9ed8d
  System UUID:                e2d398c7-3130-4bbf-a33d-6ea6ba056c4d
  Boot ID:                    7324948c-75af-4ea6-8c8b-0383ba319ca3
  Kernel Version:             5.4.0-131-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.7.13
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      192.168.0.0/24
PodCIDRs:                     192.168.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  kube-system                 calico-kube-controllers-75bdb5b75d-kh2wj    0 (0%)        0 (0%)      0 (0%)           0 (0%)         12d
  kube-system                 canal-rc7zk                                 25m (2%)      0 (0%)      0 (0%)           0 (0%)         12d
  kube-system                 etcd-controlplane                           25m (2%)      0 (0%)      100Mi (5%)       0 (0%)         12d
  kube-system                 kube-apiserver-controlplane                 50m (5%)      0 (0%)      0 (0%)           0 (0%)         12d
  kube-system                 kube-controller-manager-controlplane        25m (2%)      0 (0%)      0 (0%)           0 (0%)         12d
  kube-system                 kube-proxy-6xsx9                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         12d
  kube-system                 kube-scheduler-controlplane                 25m (2%)      0 (0%)      0 (0%)           0 (0%)         12d
  local-path-storage          local-path-provisioner-75655fcf79-s7bbc     0 (0%)        0 (0%)      0 (0%)           0 (0%)         12d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                150m (15%)  0 (0%)
  memory             100Mi (5%)  0 (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 11m                kube-proxy       
  Normal  Starting                 12d                kube-proxy       
  Normal  Starting                 12d                kube-proxy       
  Normal  Starting                 12d                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  12d                kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    12d                kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     12d                kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  12d                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           12d                node-controller  Node controlplane event: Registered Node controlplane in Controller
  Normal  NodeReady                12d                kubelet          Node controlplane status is now: NodeReady
  Normal  RegisteredNode           12d                node-controller  Node controlplane event: Registered Node controlplane in Controller
  Normal  Starting                 12d                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  12d                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  12d (x8 over 12d)  kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    12d (x8 over 12d)  kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     12d (x7 over 12d)  kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal  RegisteredNode           12d                node-controller  Node controlplane event: Registered Node controlplane in Controller
  Normal  Starting                 12m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  12m (x8 over 12m)  kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    12m (x8 over 12m)  kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     12m (x7 over 12m)  kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  12m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           11m                node-controller  Node controlplane event: Registered Node controlplane in Controller
  Normal  NodeNotReady             8m21s              node-controller  Node controlplane status is now: NodeNotReady

-Check the kubelet status
systemctl status kubelet

Output
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: inactive (dead) since Thu 2024-08-15 06:19:04 UTC; 9min ago
       Docs: https://kubernetes.io/docs/
    Process: 1723 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARG>
   Main PID: 1723 (code=exited, status=0/SUCCESS)

Aug 15 06:16:36 controlplane kubelet[1723]: E0815 06:16:36.862321    1723 pod_workers.go:1298] "Error syncing pod, skipping" err="f>
Aug 15 06:16:36 controlplane kubelet[1723]: E0815 06:16:36.910688    1723 kuberuntime_manager.go:1075] "killPodWithSyncResult faile>
Aug 15 06:16:36 controlplane kubelet[1723]: E0815 06:16:36.910730    1723 pod_workers.go:1298] "Error syncing pod, skipping" err="f>
Aug 15 06:16:38 controlplane kubelet[1723]: I0815 06:16:38.475526    1723 kuberuntime_container_linux.go:167] "No swap cgroup contr>
Aug 15 06:16:51 controlplane kubelet[1723]: I0815 06:16:51.689803    1723 kuberuntime_container_linux.go:167] "No swap cgroup contr>
Aug 15 06:17:25 controlplane kubelet[1723]: I0815 06:17:25.932382    1723 scope.go:117] "RemoveContainer" containerID="d32198297c7b>
Aug 15 06:17:25 controlplane kubelet[1723]: I0815 06:17:25.947833    1723 scope.go:117] "RemoveContainer" containerID="e7ccc675a38c>
Aug 15 06:19:04 controlplane systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Aug 15 06:19:04 controlplane systemd[1]: kubelet.service: Succeeded.
Aug 15 06:19:04 controlplane systemd[1]: Stopped kubelet: The Kubernetes Node Agent.

- Check journalctl for kubelet logs
journalctl -xeu kubelet

- Start the kubelet
systemctl start kubelet

- Check the status of kubelet (it should be running now)
systemctl status kubelet

Output
controlplane $ systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active (running) since Thu 2024-08-15 06:28:47 UTC; 2s ago
       Docs: https://kubernetes.io/docs/
   Main PID: 6885 (kubelet)
      Tasks: 9 (limit: 2338)
     Memory: 70.8M
     CGroup: /system.slice/kubelet.service
             └─6885 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kub>

Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.326358    6885 projected.go:294] Couldn't get configMap local-path-stora>
Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.326418    6885 projected.go:200] Error preparing data for projected volu>
Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.326472    6885 nestedpendingoperations.go:348] Operation for "{volumeNam>
Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.751987    6885 configmap.go:199] Couldn't get configMap local-path-stora>
Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.752033    6885 nestedpendingoperations.go:348] Operation for "{volumeNam>
Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.852928    6885 projected.go:294] Couldn't get configMap local-path-stora>
Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.852967    6885 projected.go:200] Error preparing data for projected volu>
Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.852997    6885 nestedpendingoperations.go:348] Operation for "{volumeNam>
Aug 15 06:28:50 controlplane kubelet[6885]: I0815 06:28:50.339640    6885 pod_container_deletor.go:80] "Container not found in pod'>
Aug 15 06:28:50 controlplane kubelet[6885]: I0815 06:28:50.340453    6885 scope.go:117] "RemoveContainer" containerID="ef118d86734d>
...skipping...
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active (running) since Thu 2024-08-15 06:28:47 UTC; 2s ago
       Docs: https://kubernetes.io/docs/
   Main PID: 6885 (kubelet)
      Tasks: 9 (limit: 2338)
     Memory: 70.8M
     CGroup: /system.slice/kubelet.service
             └─6885 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kub>

Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.326358    6885 projected.go:294] Couldn't get configMap local-path-stora>
Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.326418    6885 projected.go:200] Error preparing data for projected volu>
Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.326472    6885 nestedpendingoperations.go:348] Operation for "{volumeNam>
Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.751987    6885 configmap.go:199] Couldn't get configMap local-path-stora>
Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.752033    6885 nestedpendingoperations.go:348] Operation for "{volumeNam>
Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.852928    6885 projected.go:294] Couldn't get configMap local-path-stora>
Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.852967    6885 projected.go:200] Error preparing data for projected volu>
Aug 15 06:28:49 controlplane kubelet[6885]: E0815 06:28:49.852997    6885 nestedpendingoperations.go:348] Operation for "{volumeNam>
Aug 15 06:28:50 controlplane kubelet[6885]: I0815 06:28:50.339640    6885 pod_container_deletor.go:80] "Container not found in pod'>
Aug 15 06:28:50 controlplane kubelet[6885]: I0815 06:28:50.340453    6885 scope.go:117] "RemoveContainer" containerID="ef118d86734d>

- Check the nodes again (controlplane should be running)
kubectl get nodes

Output
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   12d   v1.30.0
node01         Ready    <none>          12d   v1.30.0

- Check the etcd pod under ns kube-system
kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS      AGE
calico-kube-controllers-75bdb5b75d-b7s29   1/1     Running   0             4m32s
canal-rc7zk                                2/2     Running   2 (13m ago)   12d
canal-rnmhk                                2/2     Running   2 (13m ago)   12d
coredns-5c69dbb7bd-nzg9k                   1/1     Running   1 (13m ago)   12d
coredns-5c69dbb7bd-pk85q                   1/1     Running   1 (13m ago)   12d
etcd-controlplane                          1/1     Running   2 (13m ago)   12d
kube-apiserver-controlplane                1/1     Running   2 (13m ago)   12d
kube-controller-manager-controlplane       1/1     Running   2 (13m ago)   12d
kube-proxy-6xsx9                           1/1     Running   2 (13m ago)   12d
kube-proxy-vrwqx                           1/1     Running   1 (13m ago)   12d
kube-scheduler-controlplane                1/1     Running   2 (13m ago)   12d

- Take back of etcd to file name - /opt/cluster_backup.db
ETCDCTL_API=3 etcdctl --endpoints=https://172.30.1.2:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/cluster_backup.db

Output
{"level":"info","ts":1723703394.012537,"caller":"snapshot/v3_snapshot.go:68","msg":"created temporary db file","path":"/opt/cluster_backup.db.part"}
{"level":"info","ts":1723703394.0245504,"logger":"client","caller":"v3/maintenance.go:211","msg":"opened snapshot stream; downloading"}
{"level":"info","ts":1723703394.0247169,"caller":"snapshot/v3_snapshot.go:76","msg":"fetching snapshot","endpoint":"https://172.30.1.2:2379"}
{"level":"info","ts":1723703394.113817,"logger":"client","caller":"v3/maintenance.go:219","msg":"completed snapshot read; closing"}
{"level":"info","ts":1723703394.151719,"caller":"snapshot/v3_snapshot.go:91","msg":"fetched snapshot","endpoint":"https://172.30.1.2:2379","size":"4.7 MB","took":"now"}
{"level":"info","ts":1723703394.1519723,"caller":"snapshot/v3_snapshot.go:100","msg":"saved","path":"/opt/cluster_backup.db"}
Snapshot saved at /opt/cluster_backup.db

- Check the backup status 
export ETCDCTL_API=3
etcdctl --write-out=table snapshot status /opt/cluster_backup.db  

Output 
Deprecated: Use `etcdutl snapshot status` instead.

+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| bf6a6359 |     3506 |       1495 |     4.7 MB |
+----------+----------+------------+------------+

- Now save this output to backup.txt file
etcdctl --write-out=table snapshot status /opt/cluster_backup.db > backup.txt
Deprecated: Use `etcdutl snapshot status` instead.


- Check the backup.txt file
cat backup.txt 

Output
+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| bf6a6359 |     3506 |       1495 |     4.7 MB |
+----------+----------+------------+------------+

COMPLETED

K8s Refernce Doc- https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/

ISSUE- controlplane is in running state and backup creation is also successful with backup output captured backup.txt file, still validation fails. Did I missing any steps in fixing this issue?

TRAINER INPUTS:

??

---------------------------------------------------------------------------------------------------------------------------

Question No- 7



ISSUE- 
TRAINER INPUTS:

??

----------------------------------------------------------------------------------------------------------------------------




